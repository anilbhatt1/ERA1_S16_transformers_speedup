{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_config, get_weights_file_path\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torchmetrics\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "\n",
    "config = get_config()\n",
    "config['batch_size'] = 64\n",
    "config['preload'] = None\n",
    "config['num_epochs'] = 60\n",
    "\n",
    "from train import get_model, get_ds, run_validation\n",
    "import torch\n",
    "torch.cuda.amp.autocast(enabled=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(torch.nn.functional, 'scaled_dot_product_attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'device : {device}')\n",
    "\n",
    "Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "writer = SummaryWriter(config['experiment_name'])\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LR = 10**-3\n",
    "STEPS_PER_EPOCH = len(train_dataloader)\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=MAX_LR,\n",
    "                                                steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                                epochs=EPOCHS,\n",
    "                                                pct_start=1/10 if EPOCHS !=1 else 0.5,\n",
    "                                                div_factor=10,\n",
    "                                                three_phase=True,\n",
    "                                                final_div_factor=10, \n",
    "                                                anneal_strategy='linear'\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0\n",
    "global_step = 0\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "lr = [0.0]\n",
    "\n",
    "for epoch in range(initial_epoch, EPOCHS):\n",
    "    loss_acc = []\n",
    "    torch.cuda.empty_cache()\n",
    "    model.train()\n",
    "    batch_iterator = tqdm(train_dataloader, desc=f'Processing epoch {epoch: 02d}')\n",
    "    for batch in batch_iterator:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        encoder_input = batch['encoder_input'].to(device)\n",
    "        decoder_input = batch['decoder_input'].to(device)\n",
    "        encoder_mask = batch['encoder_mask'].to(device)\n",
    "        decoder_mask = batch['decoder_mask'].to(device)  \n",
    "\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "        \n",
    "        batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "        writer.add_scalar('train_loss', loss.item(), global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        scale = scaler.get_scale()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        skip_lr_sched = (scale > scaler.get_scale())\n",
    "\n",
    "        if not skip_lr_sched:\n",
    "            scheduler.step()\n",
    "        lr.append(scheduler.get_last_lr())\n",
    "\n",
    "        global_step += 1\n",
    "\n",
    "    run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "        \n",
    "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'global_step': global_step\n",
    "    }, model_filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
